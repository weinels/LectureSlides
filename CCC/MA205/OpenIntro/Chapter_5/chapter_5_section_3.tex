\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{helvet}
\usepackage[T1]{fontenc}
\usepackage[inline]{asymptote}
\usepackage{asy_helper}
\usepackage{slide_helper}
\usepackage{multirow}
\usepackage{cancel}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{pgfplots}
\pgfplotsset{compat=1.5} 
\usepgfplotslibrary{statistics}
\usetikzlibrary{external}
\tikzexternalize%

\begin{asydef}
  real nd_func(real mu, real sigma, real x)
  {
    return 1/sqrt(2*pi*sigma*sigma)*exp((-1*(x-mu)*(x-mu))/(2*sigma*sigma));
  }
  
  guide normal_dist(real mu, real sigma, real xmin, real xmax)
  {
    real nd(real x) { return nd_func(mu, sigma, x); }
    return graph(nd, xmin, xmax);
  }

  void shade_below(real mu, real sigma, real b, real xmin, real xmax, pen p=royalblue)
  {
    real nd_func(real x) { return 1/sqrt(2*pi*sigma*sigma)*exp((-1*(x-mu)*(x-mu))/(2*sigma*sigma)); }
    
    guide g = graph(nd_func, xmin, b);
    
    filldraw(g -- (b,0) -- cycle, p, black);
    
    draw((xmin,0)--(b,0));
  }

  void shade_above(real mu, real sigma, real b, real xmin, real xmax, pen p=royalblue)
  {
    real nd_func(real x) { return 1/sqrt(2*pi*sigma*sigma)*exp((-1*(x-mu)*(x-mu))/(2*sigma*sigma)); }
    
    guide g = graph(nd_func, b, xmax);
    
    filldraw(g -- (b,0) -- cycle, p, black);
    
    draw((xmax,0)--(b,0));
  }

  void shade_between(real mu, real sigma, real a, real b, pen p=royalblue)
  {
    real nd_func(real x) { return 1/sqrt(2*pi*sigma*sigma)*exp((-1*(x-mu)*(x-mu))/(2*sigma*sigma)); }
    
    guide g = graph(nd_func, a, b);
    
    filldraw((a,0) -- g -- (b,0) -- cycle, p, black);
  }

  void multiple_nd_curves_example(real std_dev)
  {
    size(300, 190, IgnoreAspect);
    
    draw(normal_dist(0, std_dev, -6,6));
    shade_between(0,std_dev,-std_dev,std_dev);
    draw((0,0)--(0,0.45));

    label("$\sigma="+format("%#.2f", std_dev)+"$", (-4.2,0.45), Fill(paleyellow));

    xaxis(Bottom(), RightTicks(new real[] {-6,-4,-2,0,2,4,6}));
    yaxis(Left(), LeftTicks(size=nan),ymin = 0, ymax = 0.5);
  }
\end{asydef}

\begin{asydef}
  pair ex2 = (250, 70);
  pair blkpq = (90, 90);
\end{asydef}

\newcommand{\satisfied}[0]{{\color{green!70!black}\checkmark}}

\title[MA205 - Section 5.3]{Hypothesis Testing For A Proportion}

\newcommand{\prob}[1]{P\left({#1}\right)}
\newcommand{\jointprob}[3]{\prob{{#1}~\text{#2}~{#3}}}
\newcommand{\condprob}[2]{\prob{{#1}~|~{#2}}}
\newcommand{\comb}[2]{_{#1}C_{#2}}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \begin{example}\label{infant vaccination}
    \vspace{-2mm} %beamer spacing is bugged
    The following question comes from a book written by Hans Rosling, Anna Rosling R\"{o}nnlund, and Ola Rosling called \emph{Factfulness}.

    \vspace{1mm}
    \emph{How many of the world's 1 year old children today have been vaccinated against some disease?}

    \vspace{-2.5mm}
    \begin{center}
      (a) 20\%
      \qquad\quad
      (b) 50\%
      \qquad\quad
      (c) 80\%
    \end{center}

    \vspace{-2.5mm}
    \question{What is your answer (or guess)?}\pause
    \answer{The correct answer is 80\%.}\pause
  \end{example}
  
  \begin{note}
    If we take a multiple choice test, then we might like to distinguish between the two possibilities:
    \begin{itemize}
    \item People never learn these particular topics and their responses are simply equivalent to random guessing.
    \item People have knowledge that helps them do better than random guessing, or perhaps, they have false knowledge that leads them to actually do worse than random guessing.
    \end{itemize}
  \end{note}
\end{frame}

\begin{frame}
\begin{definition}
In statistics, a \textbf{hypothesis} is a claim or statement about some property of a population.
\end{definition}\pause

\begin{definition}
A \textbf{hypothesis test} (or \textbf{test of significance}) is a procedure for testing a claim about some property of a population.
\end{definition}\pause

\begin{definition}
The \textbf{null hypothesis} ($H_0$) represents a skeptical perspective or a claim to be tested.
\end{definition}\pause

\begin{definition}
The \textbf{alternative hypotheses} ($H_A$) represents an alternative claim under consideration and is often represented by a range of possible parameter values.
\end{definition}
\end{frame}

\begin{frame}
  \begin{example}
    A US court considers two possible claims about a defendant: they are either innocent or guilty.

    \vspace{1mm}
    \question{Which would be $H_0$ and which $H_A$?}\pause
    \answer{The jury considers whether the evidence is so convincing that there is no reasonable doubt regarding the person's guilt.

      \vspace{-3mm}
      \begin{equation*}
        \begin{aligned}
          H_0 &: \text{Innocence} \\
          H_A &: \text{Guilty}
        \end{aligned}
      \end{equation*}
      \vspace{-3mm}}
  \end{example}\pause

  \begin{note}
    Just because the jurors leave unconvinced of guilt beyond a reasonable doubt, does not mean they believe the defendant is innocent.\pause

    \vspace{1mm}
    We may reject or fail to reject the alternative hypothesis, but we typically never accept the null hypothesis as true.
  \end{note}
\end{frame}

\begin{frame}
  \begin{example}
    In Example~\ref{infant vaccination} we had three choices for the question:
    
    \vspace{1mm}
    \emph{How many of the world's 1 year old children today have been vaccinated against some disease?}

    \vspace{-2.5mm}
    \begin{center}
      (a) 20\%
      \qquad\quad
      (b) 50\%
      \qquad\quad
      (c) 80\%
    \end{center}\pause

    \vspace{-2mm}
    \question{If someone has pick an answer completely at random, what would the probability of selecting the correct answer be?}\pause
    \answer{$p = \dfrac{1}{3} = 0.333$}\pause

    \vspace{1mm}
    So, if the null hypothesis is that someone was guessing at random, the alternative would be they were not guessing.\pause

    \vspace{-3mm}
    \begin{equation*}
      \begin{aligned}
        H_0 &: p = 0.333 \\
        H_A &: p \neq 0.333
      \end{aligned}
    \end{equation*}
  \end{example}\pause

  \begin{definition}
    The value we are comparing the parameter to is called the \textbf{null value}.
  \end{definition}
\end{frame}

\begin{frame}
  \begin{examplecont}
    It may seem incredibly unlikely that the proportion of people who get the correct answer is \emph{exactly} 33.3\%.

    \vspace{1mm}
    \question{If we don't believe the null hypothesis, should we simply reject it?}\pause
    \answer{No. While we may not believe the null hypotheses, we need strong evidence before we reject the null hypothesis and conclude something more interesting.\pause

      \vspace{1mm}
      Even if we don't believe the proportion is \emph{exactly} 33.3\%, that doesn't tell us anything useful about if people do better or worse than random guessing.}
  \end{examplecont}
\end{frame}

\begin{frame}
  \begin{note}
    We will be using the \dataset{rosling\_responses} data set to evaluate the hypothesis test evaluating whether college-educated adults who get the question about infant vaccination correct is different from 33.3\%.\pause

    \vspace{1mm}
    This data set summarizes the answers of 50 college-educated adults. Of these 50 adults, 24\% of respondents got the question correct.
  \end{note}
\end{frame}

\begin{frame}
  \begin{example}
    For this data set, we have $n=50$ and $\hat{p}=0.24$. Lets see if it's reasonable to construct a confidence interval.\pause

    \vspace{-3mm}
    \begin{equation*}
      \begin{aligned}
        np &=\pause 50\cdot 0.24\pause = 12\pause \geq 10~\satisfied \\\pause
        n(1-p) &=\pause 50(1-0.24)\pause = 50\cdot 0.76\pause = 38\pause \geq 10~\satisfied
      \end{aligned}
    \end{equation*}\pause

    \vspace{-3mm}
    The conditions are met, so let's construct a 95\% confidence interval.

    \vspace{-6mm}
    \begin{equation*}
      \begin{aligned}
        \hat{p} \pm z^* \sqrt{\dfrac{\hat{p}(1-\hat{p})}{n}}\pause
        = 0.24 \pm 1.96\sqrt{\dfrac{0.24(1-0.24)}{5}}\pause
        = 0.24 \pm 0.118381\pause
      \end{aligned}
    \end{equation*}

    \vspace{-2mm}
    The confidence interval is $\interval{\open{0.122}}{\open{0.358}}$, which means we are 95\% confident that the proportion of college-educated adults to correctly answer the question is between 12.2\% and 35.8\%.\pause

    \vspace{1mm}
    Since the null hypothesis of $p=0.333$ falls in this range, we cannot say the null hypotheses is implausible. We fail to reject the null hypothesis.

    \vspace{1mm}
    Just because we conclude that it's plausible that $p=0.333$ does not mean we actually accept the null hypothesis.
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}
    \emph{There are 2 billion children in the world today aged 0-15, how many children will there be in 2100 according to the United Nations?}

    \vspace{-2mm}
    \begin{center}
      (a) 4 billion
      \qquad
      (b) 3 billion
      \qquad
      (c) 2 billion
    \end{center}\pause

    \vspace{-2mm}
    \question{What are the null and alternative hypotheses?}
    \answer{%
      
      \vspace{-5mm}
      \begin{equation*}
        \begin{aligned}
          H_0 : p=0.333
          \qquad
          H_A : p\neq 0.333
        \end{aligned}
      \end{equation*}
    }\pause

    \vspace{-6mm}
    If we take a larger sample of 228 college-educated adults, 34 (14.9\%) selected the correct answer: (c) 2 billion\pause

    \vspace{1mm}
    Let's construct a 95\% confidence interval.\pause

    \vspace{-6mm}
    \begin{equation*}
      \begin{aligned}
        \hat{p} \pm z^* \sqrt{\dfrac{\hat{p}(1-\hat{p})}{n}}\pause
        = 0.149 \pm 1.96 \sqrt{\dfrac{0.149(0.851)}{228}}\pause
        = 0.149 \pm 0.04622
      \end{aligned}
    \end{equation*}\pause

    \vspace{-4mm}
    The confidence interval is $\interval{\open{0.103}}{\open{0.195}}$, which means we are 95\% confident that the proportion of college-educated adults that answered the question correctly is between 10.3\% and 19.5\% Since $p=0.333$ is implausible, we reject the null hypothesis.
  \end{example}
\end{frame}

\begin{frame}
  \begin{note}
    In the last example, because the confidence interval $\interval{\open{0.103}}{\open{0.195}}$ is below p=0.333, we can conclude that college-educated adults do worse than random guessing on this question.
  \end{note}\pause

  \begin{note}
    This shows a general trend, in that many people are more pessimistic about progress than reality suggests.
  \end{note}\pause

  \begin{note}
    It is possible to come to difficult conclusions depending on the confidence level you use.\pause

    \vspace{1mm}
    A 99.9\% confidence interval for the last example is $\interval{\open{0.072}}{\open{0.227}}$, which is wider than the 95\% confidence interval.\pause

    \vspace{1mm}
    This means you always need to write what confidence level you used.
  \end{note}
\end{frame}

\begin{frame}
  \begin{definition}
    A \textbf{Type 1 Error} is rejecting $H_0$, when $H_0$ is actually true.

    \vspace{1mm}
    A \textbf{Type 2 Error} is failing to reject $H_0$,  when $H_A$ is actually true.

    \begin{center}
      \begin{tabular}{rccc}
        &&\multicolumn{2}{c}{Test conclusion}\\\cline{3-4}
        &&do not reject $H_0$&reject $H_0$ in favor of $H_A$ \\\cline{2-4}
        \multirow{2}{*}{Truth} & $H_0$ true & okay & \textbf{Type 1 Error} \\
        & $H_A$ true & \textbf{Type 2 Error} & okay \\\cline{2-4}
      \end{tabular}
    \end{center}
  \end{definition}\pause

  \begin{example}
    In a US court, the defendant is either innocent ($H_0$) or guilty ($H_A$).

    \vspace{1mm}
    \question{What does a Type 1 Error represent in this context?}\pause
    \answer{The defendant is innocent, but wrongly convicted.}\pause

    \vspace{1mm}
    \question{What does a Type 2 Error represent in this context?}\pause
    \answer{The defendant really did commit the crime, but was not found guilty.}
  \end{example}
\end{frame}

\begin{frame}
  \begin{note}
    Confidence intervals quickly get clunky, so we will talk about a tool that will let us work with more complicated data scenarios.
  \end{note}\pause
  
  \begin{definition}
    The \textbf{p-value} is the probability of observing data at least as favorable to the alternative hypothesis as out current data set, if the null hypothesis were true.
  \end{definition}\pause

  \begin{note}
    We typically use a summary statistic of the data, in this section the sample proportion, to help compute the p-value and evaluate the hypotheses.
  \end{note}
\end{frame}

\begin{frame}
  \begin{example}
    Pew Research asked a random sample of 1000 American adults whether the supported the increased usage of coal to produce energy.\pause

    \vspace{1mm}
    The uninteresting result is that there is no majority either way: Half of Americans support and the other half oppose expanding the use of coal to produce energy.

    \vspace{-2mm}
    \begin{equation*}
      \begin{aligned}
        H_0: p=0.5
      \end{aligned}
    \end{equation*}\pause

    \vspace{-4mm}
    The alternative hypothesis would be that the majority either supports or opposes expanding the use of coal to produce energy, though we don't know which one.

    \vspace{-2mm}
    \begin{equation*}
      \begin{aligned}
        H_A: p\neq 0.5
      \end{aligned}
    \end{equation*}
  \end{example}\pause

  \begin{note}
    In this case, the null value would be $p_0=0.5$.
  \end{note}
\end{frame}

\begin{frame}
  \begin{examplecont}
    Pew Research's sample show that 37\% of American adults support the increased usage of coal.\pause

    \vspace{1mm}
    It is natural to wonder if 37\% represents a real difference from the null hypothesis of 50\%. That is, if we assume the null hypothesis were true, how likely is a sample with $\hat{p}=0.37$?\pause

    \vspace{1mm}
    \textbf{Independence.} The poll was based on a random sample.\pause

    \vspace{1mm}
    \textbf{Success-Failure.} Based on the poll's sample size $n=1000$:

    \vspace{-3mm}
    \begin{equation*}
      \begin{aligned}
        np\pause &\overset{H_0}{=} 1000\cdot 0.5\pause = 500\pause \geq 10\\\pause
        n(1-p)\pause &\overset{H_0}{=} 1000(1-0.5)\pause = 500\pause \geq 10
      \end{aligned}
    \end{equation*}

    \vspace{-1mm}
    So, if the null hypothesis is true, the sampling distribution would be approximately normal, with:

    \vspace{-3mm}
    \begin{equation*}
      \begin{aligned}
        \mu_{\hat{p}} &= p\pause \overset{H_0}{=} 0.5 \\\pause
        SE_{\hat{p}} &= \sqrt{\dfrac{p(1-p)}{n}}\pause \overset{H_0}{=} \sqrt{\dfrac{0.5(1-0.5)}{1000}}\pause = 0.016
      \end{aligned}
    \end{equation*}
  \end{examplecont}
\end{frame}

\begin{frame}[fragile]
  \begin{definition}
    When we identify the sampling distribution under the null hypothesis, it is called the \textbf{null distribution}.
  \end{definition}\pause

  \begin{note}
    The p-value represents the probability of the observed $\hat{p}$, or a $\hat{p}$ that is more extreme, if the null hypothesis is true.

    \begin{center}
      \begin{asy}
        size(300, 55, IgnoreAspect);

        real mu = 0.50;
        real sigma = 0.016;

        real left = mu-1.2*sigma;
        real right = mu+1.2*sigma;

        real xmin=mu-2.5*sigma; real xmax=mu+2.5*sigma;

        shade_between(mu,sigma,xmin,left, red);
        shade_between(mu,sigma,right,xmax, red);
        draw(normal_dist(mu, sigma, xmin, xmax));

        xaxis(Bottom(), RightTicks(new string(real x) {
          if (x == left) { return "$-z_{\hat{p}}$"; }
          if (x == right) { return "$z_{\hat{p}}$"; }
          if (x == mu) { return "$0$"; }
          return "";}, new real[] {left, mu, right}));
      \end{asy}
    \end{center}
  \end{note}
\end{frame}

\begin{frame}[fragile]
  \begin{examplecont}
    With $\mu=0.5$ and $\sigma=0.016$, the $z$-value for $\hat{p}=0.37$ is:

    \vspace{-2.5mm}
    \begin{equation*}
      \begin{aligned}
        z=\dfrac{x-\mu}{\sigma}\pause
        =\dfrac{0.37-0.5}{0.016}\pause
        =-8.125
      \end{aligned}
    \end{equation*}\pause

    \vspace{-0.5mm}
    This gives the $z$-value for the left tail. But, we also need to consider being as extreme, but on the right, $z=8.125$.\pause

    \vspace{-3mm}
    \begin{equation*}
      \begin{aligned}
        x=\mu+z\sigma\pause
        =0.5+8.125\cdot0.016\pause
        =0.63
      \end{aligned}
    \end{equation*}\pause

    \vspace{-0.5mm}
    So, we need to find the area that is either to the left of 0.37 or to the right of 0.63.

    \vspace{-4mm}
    \begin{center}
      \begin{asy}
        size(300, 50, IgnoreAspect);

        real mu = 0.50;
        real sigma = 0.016;

        real left = 0.37;
        real right = 0.63;

        real xmin=mu-11*sigma; real xmax=mu+11*sigma;

        //shade_between(mu,sigma,xmin,xmax);
        draw(normal_dist(mu, sigma, xmin, xmax));

        dot((left,0), red);
        draw((xmin,0) -- (left,0), red+1bp);
        
        dot((right,0), red);
        draw((right,0) -- (xmax,0), red+1bp);

        xaxis(Bottom(), RightTicks(new real[] {left, mu, right}));
      \end{asy}
    \end{center}\pause

    \vspace{-2mm}
    The probability of falling into the red area is:

    \vspace{-4mm}
    \begin{equation*}
      \begin{aligned}
        \jointprob{z\leq -8.125}{or}{z\geq 8.125}\pause = 4.4\times10^{-16}
      \end{aligned}
    \end{equation*}
  \end{examplecont}
\end{frame}

\begin{frame}
  \begin{note}
    The p-value of $4.4\times10^{-16}$ gives two possible scenarios:\pause
    \begin{enumerate}
    \item The null hypothesis is true, and we just happened to observe something so extreme that it only happens once in every 23 quadrillion times.\pause
    \item The alternative hypothesis is true, which would be consistent with observing a sample proportion far from 0.5.
    \end{enumerate}\pause

    The first scenario is so improbable, that it's much more likely the second scenario is true.
  \end{note}\pause

  \begin{block}{Inferential Statistics}
      If, under an given assumption, the probability of a particular observed event is very small, we conclude that the assumption is probably nor correct.
  \end{block}
\end{frame}

\begin{frame}
  \begin{block}{Hypothesis Testing for a Single Proportion}
    Once you've determined a one-proportion hypothesis test is the correct procedure:\pause
    \begin{description}
    \item[Prepare:] Identify the parameter of interest, list hypotheses, identify the significance level $\alpha$, and identify $\hat{p}$ and $n$.\pause
    \item[Check:] Verify conditions to ensure $\hat{p}$ is nearly normal under $H_0$. Use the null-value to check the success-failure condition.\pause
    \item[Calculate:] Using the null value, compute the standard error, the $z$-score, and the p-value.\pause
    \item[Conclude:] If the p-value is less than $\alpha$, reject $H_0$. Otherwise, fail to reject $H_0$.
    \end{description}
  \end{block}\pause

  \begin{note}
    Make sure to provide a conclusion in the context of the problem. Most people don't understand what rejecting or failing to reject $H_0$ means.
  \end{note}
\end{frame}

\begin{frame}[fragile]
  \begin{example}
    A simple random sample of 1028 US adults in March 2013 show that 56\% support nuclear arms reduction.\pause

    \vspace{1mm}
    Does this provide convincing evidence that a majority of Americans supported nuclear arms reduction at the 5\% significance level?\pause

    \vspace{1mm}
    We have $\alpha=0.05$, $\hat{p}=0.56$, and $n=1028$.\pause

    \vspace{1mm}
    The hypotheses are $H_0: p = 0.5$ and $H_A: p\neq 0.5$.\pause

    \vspace{1mm}
    Next, we check the success-failure conditions using the null value:

    \vspace{-4mm}
    \begin{equation*}
      \begin{aligned}
        np = n(1-p) =\pause 1028\cdot 0.5\pause = 514 \geq 10
      \end{aligned}
    \end{equation*}\pause

    \vspace{-6mm}
    Next the standard error and $z$-score:

    \vspace{-2mm}
    \begin{equation*}
      \begin{aligned}
        SE_{\hat{p}} &\overset{H_0}{=}\pause \sqrt{p(1-p)/n}\pause = \sqrt{0.5(1-0.5)/1028}\pause =0.0156 \\\pause
        z &= (\hat{p}-\text{null value})/SE_{\hat{p}}\pause = (0.56-0.5)/0.0156\pause = 3.85
      \end{aligned}
    \end{equation*}\pause

    \vspace{-4mm}
    \begin{center}
      \begin{asy}
        size(200, 33, IgnoreAspect);

        real mu = 0.0;
        real sigma = 1;

        real left = -3.85;
        real right = 3.85;

        real xmin=mu-4.75*sigma; real xmax=mu+4.75*sigma;

        //shade_between(mu,sigma,xmin,xmax);
        draw(normal_dist(mu, sigma, xmin, xmax));

        dot((left,0), red);
        draw((xmin,0) -- (left,0), red+1bp);
        
        dot((right,0), red);
        draw((right,0) -- (xmax,0), red+1bp);

        xaxis(Bottom(), RightTicks(new string(real x) { if (x != 0) { return "$z=" + string(x) + "$"; } return string(x); }, new real[] {left, mu, right}));
      \end{asy}
    \end{center}

    \vspace{-4mm}
    The p-value is 0.0002,\pause~which is smaller than $\alpha=0.05$, so we reject $H_0$.
  \end{example}
\end{frame}

\begin{frame}
  \begin{block}{Choosing a Significance Level}
    Traditionally a significance level of $\alpha=0.05$ is used.\pause

    \vspace{1mm}
    If making a Type I Error is dangerous or costly, we want to be very cautious about rejecting the null hypothesis, and we should choose a smaller significance level. (i.e. $\alpha=0.01$)\pause

    \vspace{1mm}
    If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we want to be very cautious about failing to reject the null hypothesis, and we should choose a larger significance level. (i.e. $\alpha=0.1$)
  \end{block}\pause

  \begin{note}
    Type 2 Errors can be reduced by collecting more data, without affecting Type I Errors. But this often adds time and expense to a study. There is typically a cost-befit analysis to be considered.
  \end{note}
\end{frame}

\begin{frame}
  \begin{example}
    A car manufacturer is considering switching to a new, higher quality piece of equipment that constructs vehicle door hinges.\pause

    \vspace{1mm}
    They figure that they will save money in the long run if this new machine produces hinges that have flaws less than 0.2\% of the time.\pause

    \vspace{1mm}
    However, if the hinges are flawed more than 0.2\% of the time, they wouldn't get a good enough return-on-investment from the new piece of equipment.\pause

    \vspace{1mm}
    The hypotheses are:

    \vspace{-2mm}
    \begin{equation*}
      \begin{aligned}
        H_0 &: \text{Rate of flawed hinges is 0.2\%} \\
        H_A &: \text{Rate of flawed hinges is different than 0.2\%}
      \end{aligned}
    \end{equation*}\pause
    
    \vspace{-2mm}
    \question{What significance level should be used in such a hypothesis test?}\pause
    \answer{Since neither Type 1 nor Type 2 errors should be dangerous or, relatively, much more expensive, 5\% would be a reasonable choice.}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}
    The same car manufacturer is considering a slightly more expensive supplier for parts related to safety, instead of door hinges.\pause

    \vspace{1mm}
    If the durability of these safety components is shown to be better than the current supplier, they will switch manufacturers.\pause

    \vspace{1mm}
    The hypotheses are:

    \vspace{-2mm}
    \begin{equation*}
      \begin{aligned}
        H_0 &: \text{The parts are equally reliable.} \\
        H_A &: \text{The parts are not equally reliable.}
      \end{aligned}
    \end{equation*}\pause
    
    \vspace{-2mm}
    \question{What significance level should be used in such a hypothesis test?}\pause
    \answer{Since safety is involved, the car manufacturer should be eager to switch to the slightly more expensive supplier. A slightly larger significance level, such as $\alpha=0.1$, might be appropriate.}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}
    A part in a machine is very expensive to replace. However, the machine usually functions properly even if this part is broken.\pause

    \vspace{1mm}
    This means that the part is only replaces if we are extremely certain it is broken based on a series of measurements.\pause

    \vspace{1mm}
    The hypotheses are:

    \vspace{-2mm}
    \begin{equation*}
      \begin{aligned}
        H_0 &: \text{The part is not broken.} \\
        H_A &: \text{The part is broken.}
      \end{aligned}
    \end{equation*}\pause

    \vspace{-2mm}
    \question{What significance level should be used in such a hypothesis test?}\pause
    \answer{Since it doesn't seem to be a major problem if the part stays broken, we require very strong evidence before we pay to replace the part. A small significance, such as $\alpha=0.01$, might be appropriate.}
  \end{example}
\end{frame}

\begin{frame}
  \begin{note}
    So far, we have considered what are called \textbf{two-sided hypothesis tests}, where are care about detecting whether $p$ is either above for below some null value.
  \end{note}\pause

  \begin{definition}
    A hypothesis test is called \textbf{one-sided} if it takes one of following forms:\pause
    \begin{itemize}
    \item There's only one value detecting if the population parameter is \emph{less than} some value $p_0$. In this case, the alternative hypothesis is written $p<p_0$ for some null value $p_0$.\pause
    \item There's only one value detecting if the population parameter is \emph{more than} some value $p_0$. In this case, the alternative hypothesis is written $p>p_0$ for some null value $p_0$.\pause
    \end{itemize}
  \end{definition}

  \begin{note}
    When using a one-sided hypothesis test, there is a risk of overlooking data supporting the opposite conclusion.
  \end{note}
\end{frame}

\begin{frame}[fragile]
  \begin{block}{Two-sided Test $\left(H_A:\neq\right)$}
    The critical region is in the two extreme regions under the curve.
    \begin{center}
      \begin{asy}
        size(150,33,IgnoreAspect);
        real xmin=-4; 
        real xmax=4;

        real a=-1.25;
        real b=1.25;

        draw(normal_dist(0,1,xmin,xmax));
        shade_between(0,1,xmin,a,red);
        shade_between(0,1,b,xmax,red);

        real y = nd_func(0,1,0)/5;
        draw((a-0.25,y)--(b+0.25,y),Arrows(TeXHead));
        label("p-value",(0,y),UnFill);

        xaxis();
      \end{asy}
    \end{center}
  \end{block}\pause

  \begin{block}{Left-sided Test $\left(H_A:<\right)$}
    The critical region is in the extreme left region under the curve.
    \begin{center}
      \begin{asy}
        size(150,33,IgnoreAspect);
        real xmin=-4; 
        real xmax=4;

        real a=-1.25;
        real b=1.25;

        draw(normal_dist(0,1,xmin,xmax));
        shade_between(0,1,xmin,a,red);

        real y = nd_func(0,1,0)/5;
        draw((a-0.25,y)--(0,y),Arrows(TeXHead));
        label("p-value",(0,y),UnFill);

        xaxis();
      \end{asy}
    \end{center}
  \end{block}\pause

  \begin{block}{Right-sided Test $\left(H_A:>\right)$}
    The critical region is in the extreme right region under the curve.
    \begin{center}
      \begin{asy}
        size(150,33,IgnoreAspect);
        real xmin=-4; 
        real xmax=4;

        real a=-1.25;
        real b=1.25;

        draw(normal_dist(0,1,xmin,xmax));
        shade_between(0,1,b,xmax,red);

        real y = nd_func(0,1,0)/5;
        draw((0,y)--(b+0.25,y),Arrows(TeXHead));
        label("p-value",(0,y),UnFill);

        xaxis();
      \end{asy}
    \end{center}
  \end{block}
\end{frame}

\begin{frame}
  \begin{example}
    In section 1.1, we considered a study where doctors were interested in determining whether stents would help people who had a high risk of strokes.\pause

    \vspace{1mm}
    The researchers believed the stents would help, unfortunately, the data showed the opposite.\pause

    \vspace{1mm}
    If the researchers had used a one-sided test, they would have limited their ability to identify the potential harm to patients.
  \end{example}\pause

  \begin{note}
    You should use one-sided tests very rarely.\pause

    \vspace{1mm}
    When considering one, ask yourself:
    \begin{center}
      \emph{What would I, or others, conclude if the data happens to go clearly in the opposite direction than my alternative hypothesis?}
    \end{center}
  \end{note}
\end{frame}
\end{document}
